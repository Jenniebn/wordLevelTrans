# Aligning Embedding Spaces Across Languages to Identify Word Level Equivalents
Josephine Kaminaga, Jennie Wu, Daniel Yeung

We investigated the possibility of word level machine translation using pretrained embeddings on a large-scale text dataset. In building word level embedding based encoder-decoder models, we are making an exploratory attempt to investigate how practical context-free translation can be. 

Through our models we will provide a simple baseline method of developing word level translation systems, and recognize potential advancements that could be made in the field. Our research has shown promising results and will try to improve the implementation further in the future.

