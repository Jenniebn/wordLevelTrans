{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LING 111 Project: ENG-CHN word to word translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping a dictionary: rudimentary steps\n",
    "# First, request a website!\n",
    "# For this one, I'll request the Cambridge ENG-CHN dictionary\n",
    "# Having checked https://dictionary.cambridge.org/robots.txt to make sure I'm legally scraping the website\n",
    "# We can continue!\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Putting website content into a request object\n",
    "# Cambridge gives us a ConnectionError when scraping without an agent\n",
    "# So we're using a random user agent to avoid this problem\n",
    "r = requests.get(\"https://dictionary.cambridge.org/dictionary/english-chinese-simplified/peacock\", headers={\n",
    "\"User-Agent\" : \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"\n",
    "})\n",
    "# Otherwise we could just have done this:\n",
    "#r = requests.get(\"https://dictionary.cambridge.org/dictionary/english-chinese-simplified/person\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTranslation(tlString):\n",
    "    # Takes in a string tlString, formatted as such:\n",
    "    # <span class='trans dtrans dtrans-se break-cj' \n",
    "    # lang='zh-Hans'> [chinese translation equivalent] </span>\n",
    "    # Returns the most relevant Chinese translation\n",
    "    import re\n",
    "\n",
    "    # Using regex to get [chinese translation equivalent]\n",
    "    match = re.findall(r\"(?<=>).*?(?=<)\", tlString)[0]\n",
    "\n",
    "    paren1 = match.find(\"（\")\n",
    "    paren2 = match.find(\"）\")\n",
    "\n",
    "    # Removes parenthesis section\n",
    "    while paren1 != -1:\n",
    "        if paren2 == -1:\n",
    "            paren2 = len(match)\n",
    "        match = match[:paren1] + match[paren2+1:]\n",
    "        paren1 = match.find(\"（\")\n",
    "        paren2 = match.find(\"）\")\n",
    "    \n",
    "    ellipsis = match.find(\"…\")\n",
    "    if ellipsis != -1:\n",
    "        match = match[:ellipsis] + match[ellipsis+1:]\n",
    "\n",
    "    slash = match.find(\"／\")\n",
    "    if slash != -1:\n",
    "        match = match[:slash]\n",
    "    comma = match.find(\"，\")\n",
    "\n",
    "    # Removing any commas or semicolons\n",
    "    # We just want the first entry, which occurs before the commas/semicolons\n",
    "    if comma != -1:\n",
    "        match = match[:comma]\n",
    "\n",
    "    comma2 = match.find(\"、\")\n",
    "    if comma2 != -1:\n",
    "        match = match[:comma2]\n",
    "\n",
    "    semic = match.find(\"；\")\n",
    "    if semic != -1:\n",
    "        match = match[:semic]\n",
    "\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"trans dtrans dtrans-se break-cj\" lang=\"zh-Hans\"><a class=\"Ref\" href=\"/dictionary/chinese-simplified-english/\"><span class=\"dtrans\">孔雀</span></a></span>, <span class=\"trans dtrans dtrans-se break-cj\" lang=\"zh-Hans\"><a class=\"Ref\" href=\"/dictionary/chinese-simplified-english/\"><span class=\"dtrans\">注重仪表和衣饰的男子</span></a>，<a class=\"Ref\" href=\"/dictionary/chinese-simplified-english/\"><span class=\"dtrans\">虚荣骄傲的男子</span></a></span>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Okay, so we have our entire HTML page. But it's somewhat messy.\n",
    "# We can use BeautifulSoup to clean it up.\n",
    "\n",
    "# This puts our page content into proper format.\n",
    "dictContent = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "# How do we get our desired Chinese equivalent from this?\n",
    "# Having inspected the format of the dictionary, we can see that the Chinese translation of an inputted word\n",
    "# is best found in the HTML part that starts with \"<span class=\"trans dtrans dtrans-se break-cj\" ...\"\n",
    "# Thus, we will use the find method with the class_ kwarg in order to get the first instance\n",
    "# of this class in the HTML\n",
    "# Which will give us the <span> that covers the translation we're looking for!\n",
    "equivs = dictContent.find_all(class_=\"trans dtrans dtrans-se break-cj\", limit=3)\n",
    "\n",
    "print(str(equivs))\n",
    "# myString = str(dictContent.find(class_=\"trans dtrans dtrans-se break-cj\"))\n",
    "\n",
    "# Using our defined getTranslation function\n",
    "# chnEquiv = getTranslation(myString)\n",
    "# print(chnEquiv)\n",
    "mylist = getTranslation(str(equivs))\n",
    "print(mylist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our full scraper script for the Cambridge online dictionary!\n",
    "# VERSION 1\n",
    "# Actually, I used a modified version that split the ENG words dataset into\n",
    "# batches so that I could run the scrapers in parallel. They accomplish\n",
    "# the same thing as this one large chunk of code.\n",
    "\n",
    "import json\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Load english text set\n",
    "engWords = []\n",
    "with open(\"../../english_lexicon.txt\") as in_file:\n",
    "    for line in in_file:\n",
    "        engWords.append(line.strip())\n",
    "\n",
    "engChnDict = dict()\n",
    "url = \"https://dictionary.cambridge.org/dictionary/english-chinese-simplified/\"\n",
    "\n",
    "# Iterating over every word in our text set\n",
    "for word in engWords:\n",
    "    # Creating URL to query\n",
    "    wordUrl = f\"https://dictionary.cambridge.org/dictionary/english-chinese-simplified/{word}\"\n",
    "    \n",
    "    # Getting HTMl, formatting, finding translation segment\n",
    "    try:\n",
    "        r = requests.get(wordUrl, headers={\"User-Agent\" : \"Mozilla/5.0 (X11; Linux x86_64) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"})\n",
    "    except HTTPError as err:\n",
    "        continue\n",
    "    rhtml = BeautifulSoup(r.content, 'html.parser')\n",
    "    tlString = rhtml.find(class_=\"trans dtrans dtrans-se break-cj\")\n",
    "\n",
    "    # If word isn't in dictionary, rhtml.find returns None\n",
    "    # as there is no trans dtrans dtrans-se break-cj class, since\n",
    "    # Cambridge sends us to the homepage when a word isn't in the dictionary\n",
    "    if tlString == None:\n",
    "        continue\n",
    "    else: \n",
    "        # Word is in dictionary, so get the translation\n",
    "        tlString = str(tlString)\n",
    "        zhEquiv = getTranslation(tlString)\n",
    "\n",
    "    engChnDict[word] = zhEquiv\n",
    "\n",
    "# Pushing the whole dictionary to a JSON file!\n",
    "with open(\"../JSON Data/cambridgedict.json\", \"w\", encoding='utf-8-sig') as out_file:\n",
    "    json.dump(engChnDict, out_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "腹\n"
     ]
    }
   ],
   "source": [
    "# Post-processing the data sets\n",
    "def processWord(match):\n",
    "    import re\n",
    "    if match==\"\":\n",
    "        return None\n",
    "    \n",
    "    index = re.search('[a-zA-Z]', match)\n",
    "    if index != None:\n",
    "        return None\n",
    "\n",
    "    # Removes parenthesis section\n",
    "    match = re.sub(\"[\\（\\[].*?[\\）\\]]\", \"\", match)\n",
    "\n",
    "    # Removing other non-relevant charascters that show up\n",
    "    match = re.sub('[《》]', '', match)\n",
    "        # match = match[:bracket1] + match[bracket1+1:bracket2] + match[bracket2+1:]\n",
    "\n",
    "    colon = match.find(\"：\")\n",
    "    if colon != -1:\n",
    "        match = match[:colon]\n",
    "    \n",
    "    match = re.sub('[…]', '', match)\n",
    "\n",
    "    slash = match.find(\"／\")\n",
    "    if slash != -1:\n",
    "        match = match[:slash]\n",
    "\n",
    "    # Removing any commas or semicolons\n",
    "    # We just want the first entry, which occurs before the commas/semicolons\n",
    "    comma = match.find(\"，\")\n",
    "    if comma != -1:\n",
    "        match = match[:comma]\n",
    "\n",
    "    comma2 = match.find(\"、\")\n",
    "    if comma2 != -1:\n",
    "        match = match[:comma2]\n",
    "\n",
    "    semic = match.find(\"；\")\n",
    "    if semic != -1:\n",
    "        match = match[:semic]\n",
    "\n",
    "    semic2 = match.find(\"; \")\n",
    "    if semic2 != -1:\n",
    "        match = match[:semic2]\n",
    "\n",
    "    return match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knitting the scraped JSON dicts together\n",
    "import json\n",
    "dictList = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    with open(f\"../JSON Data/cambridgedict three senses {i}.json\", encoding=\"utf-8-sig\") as in_file:\n",
    "        cdict = json.load(in_file)\n",
    "        dictList.append(cdict)\n",
    "\n",
    "fullCDict = dictList[0] | dictList[1]\n",
    "\n",
    "for i in range(2, 5):\n",
    "    fullCDict = fullCDict | dictList[i]\n",
    "\n",
    "with open(\"../JSON Data/full cdict three senses.json\", \"w\", encoding=\"utf-8-sig\") as out_file:\n",
    "    json.dump(fullCDict, out_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing the text\n",
    "\n",
    "with open(\"../JSON Data/full cdict three senses.json\", encoding=\"utf-8-sig\") as in_file:\n",
    "    cdict = json.load(in_file)\n",
    "\n",
    "toDel = []\n",
    "engWords = list(cdict.keys())\n",
    "chnWords = list(cdict.values())\n",
    "\n",
    "# Iterating over all Chinese words in the dictionary\n",
    "# Processing them, appending the english key counterpart to a list\n",
    "# if the word is empty\n",
    "# and reassigning the key to the processed value otherwise\n",
    "for i in range(len(chnWords)):\n",
    "    totalProcessed = [word for word in chnWords[i] if word != None]\n",
    "    if len(totalProcessed) == 0:\n",
    "        toDel.append(engWords[i])\n",
    "    else:\n",
    "        engWord = engWords[i]\n",
    "        cdict[engWord] = totalProcessed\n",
    "\n",
    "for word in toDel:\n",
    "    del cdict[word]\n",
    "\n",
    "with open(\"../JSON Data/processed cdict three senses.json\", \"w\", encoding=\"utf-8-sig\") as out_file:\n",
    "    json.dump(cdict, out_file, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
